from __future__ import annotations

import json
import os
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Callable

from .google_docs import DocContent, DocSection
from .stable_ids import sha256_hex


@dataclass(frozen=True)
class LlmConfig:
    model: str
    max_tokens: int
    temperature: float
    cache_dir: str
    prompt_version: str


def cache_key(
    *,
    model: str,
    temperature: float,
    prompt_version: str,
    stable_doc_id: str,
    prompt: str,
) -> str:
    payload = f"{model}:{temperature}:{prompt_version}:{stable_doc_id}:{prompt}"
    return sha256_hex(payload)


def load_cache(cache_dir: str, key: str) -> DocContent | None:
    path = Path(cache_dir) / f"{key}.json"
    if not path.exists():
        return None
    data = json.loads(path.read_text())
    return _parse_doc_content(data)


def write_cache(cache_dir: str, key: str, content: DocContent, raw_text: str) -> None:
    path = Path(cache_dir)
    path.mkdir(parents=True, exist_ok=True)
    payload = {
        "title": content.title,
        "summary": content.summary,
        "sections": [
            {
                "heading": section.heading,
                "paragraphs": list(section.paragraphs),
                "bullets": list(section.bullets),
            }
            for section in content.sections
        ],
        "metadata": list(content.metadata),
        "raw_text": raw_text,
    }
    (path / f"{key}.json").write_text(json.dumps(payload, indent=2, sort_keys=True))


def generate_doc_content(
    *,
    config: LlmConfig,
    stable_doc_id: str,
    archetype: str,
    company_name: str,
    department: str,
    title_hint: str,
    run_name: str,
    regen: bool,
    generator: Callable[[str], str] | None = None,
) -> DocContent:
    prompt = build_prompt(
        archetype=archetype,
        company_name=company_name,
        department=department,
        title_hint=title_hint,
        run_name=run_name,
        prompt_version=config.prompt_version,
    )
    key = cache_key(
        model=config.model,
        temperature=config.temperature,
        prompt_version=config.prompt_version,
        stable_doc_id=stable_doc_id,
        prompt=prompt,
    )
    if not regen:
        cached = load_cache(config.cache_dir, key)
        if cached:
            return cached
    raw_text = ""
    if generator:
        raw_text = generator(prompt)
    else:
        raw_text = _call_openai(prompt, config)
    content = _parse_doc_content_from_text(raw_text, fallback_title=title_hint)
    metadata = [
        f"Synthetic document generated by gwsynth run '{run_name}'.",
        f"Archetype: {archetype}.",
    ]
    content = DocContent(
        title=content.title,
        summary=content.summary,
        sections=content.sections,
        metadata=tuple(metadata),
    )
    write_cache(config.cache_dir, key, content, raw_text)
    return content


def build_prompt(
    *,
    archetype: str,
    company_name: str,
    department: str,
    title_hint: str,
    run_name: str,
    prompt_version: str,
) -> str:
    return (
        "You are generating synthetic internal Google Docs content for a fictional company. "
        "Return strictly valid JSON with keys: title, summary, sections. "
        "sections is a list of objects with heading, paragraphs (list of strings), "
        "bullets (list of strings). "
        "Keep the content realistic for enterprise work but ensure it is fully synthetic and "
        "privacy-safe. "
        f"Archetype: {archetype}. Company: {company_name}. Department: {department}. "
        f"Title hint: {title_hint}. Run: {run_name}. Prompt version: {prompt_version}."
    )


def _parse_doc_content_from_text(text: str, fallback_title: str) -> DocContent:
    try:
        data = json.loads(text)
        return _parse_doc_content(data)
    except Exception:
        section = DocSection(
            heading="Overview",
            paragraphs=(text.strip() or "Synthetic content placeholder.",),
            bullets=(),
        )
        return DocContent(
            title=fallback_title or "Synthetic Document",
            summary="Synthetic summary.",
            sections=(section,),
            metadata=(),
        )


def _parse_doc_content(data: Any) -> DocContent:
    if not isinstance(data, dict):
        raise ValueError("Invalid LLM content payload")
    title = str(data.get("title") or "Synthetic Document")
    summary = str(data.get("summary") or "Synthetic summary.")
    sections_raw = data.get("sections", [])
    sections: list[DocSection] = []
    if isinstance(sections_raw, list):
        for item in sections_raw:
            if not isinstance(item, dict):
                continue
            heading = str(item.get("heading") or "Section")
            paragraphs = tuple(
                str(p).strip() for p in (item.get("paragraphs") or []) if str(p).strip()
            )
            bullets = tuple(
                str(b).strip() for b in (item.get("bullets") or []) if str(b).strip()
            )
            if paragraphs or bullets:
                sections.append(DocSection(heading=heading, paragraphs=paragraphs, bullets=bullets))
    if not sections:
        sections.append(
            DocSection(
                heading="Overview",
                paragraphs=("Synthetic content placeholder.",),
                bullets=(),
            )
        )
    metadata_raw = data.get("metadata", [])
    if isinstance(metadata_raw, list):
        metadata = tuple(str(m).strip() for m in metadata_raw if str(m).strip())
    else:
        metadata = ()
    return DocContent(title=title, summary=summary, sections=tuple(sections), metadata=metadata)


def _call_openai(prompt: str, config: LlmConfig) -> str:
    if not os.environ.get("OPENAI_API_KEY"):
        return _template_output(prompt)
    openai = _load_openai()
    client = openai.OpenAI()
    response = client.responses.create(
        model=config.model,
        input=prompt,
        max_output_tokens=config.max_tokens,
        temperature=config.temperature,
    )
    output_text = getattr(response, "output_text", None)
    if isinstance(output_text, str) and output_text.strip():
        return output_text
    output = getattr(response, "output", [])
    if isinstance(output, list):
        for item in output:
            content = item.get("content") if isinstance(item, dict) else None
            if isinstance(content, list):
                for chunk in content:
                    text = chunk.get("text") if isinstance(chunk, dict) else None
                    if isinstance(text, str) and text.strip():
                        return text
    return _template_output(prompt)


def _template_output(prompt: str) -> str:
    _ = prompt
    template = {
        "title": "Synthetic Document",
        "summary": "Synthetic summary for a generated document.",
        "sections": [
            {
                "heading": "Overview",
                "paragraphs": ["This is a placeholder synthetic document body."],
                "bullets": ["Replace with real synthetic content when LLM is configured."],
            }
        ],
    }
    return json.dumps(template)


def _load_openai() -> Any:
    import importlib

    return importlib.import_module("openai")
